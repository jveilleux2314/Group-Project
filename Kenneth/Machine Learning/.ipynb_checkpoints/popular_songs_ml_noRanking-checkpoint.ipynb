{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from path import Path\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable equal to loaded dataset\n",
    "tracks_df = pd.read_csv('popular_songs.csv', error_bad_lines=False)\n",
    "tracks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data and transform as needed before training for the Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe and transpose the dataframe\n",
    "tracks_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of white space before and after each column name\n",
    "tracks_df.columns = tracks_df.columns.str.strip()\n",
    "tracks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed for the ML Model\n",
    "new_tracks_df = tracks_df.drop(['song','artist','release_date','genres','popular_date'], axis =1)\n",
    "\n",
    "# Drop the null columns where all values are null\n",
    "new_tracks_df = new_tracks_df.dropna(axis='columns', how='all')\n",
    "\n",
    "# Drop the null rows\n",
    "new_tracks_df = new_tracks_df.dropna()\n",
    "new_tracks_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the weeks_on_board column\n",
    "wob_df = pd.DataFrame(new_tracks_df['weeks_on_board'])\n",
    "# Use .describe to find the average amount of time that a song remains on the top 100 Billboard\n",
    "wob_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box-and-whisker plot for the number of weeks on the board.\n",
    "# Outliers are values that are above 36 weeks on the board.\n",
    "\n",
    "x_labels = [\"Weeks On Board\"]\n",
    "fig, ax = plt.subplots(figsize=(20,26))\n",
    "ax.boxplot(wob_df, labels=x_labels)\n",
    "# Add the title, y-axis label and grid\n",
    "ax.set_title(\"Top 100 Billboard\")\n",
    "ax.set_ylabel(\"Number of weeks\")\n",
    "ax.set_yticks(np.arange(0, 100, step=1.0))\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds a new column with the name 'target' at the end of the dataframe\n",
    "header_list = ['popularity', 'acousticness', 'danceability', \n",
    "               'energy', 'instrumentalness', 'liveness', 'loudness', \n",
    "               'speechiness', 'tempo', 'ranking', 'weeks_on_board', 'target']\n",
    "new_tracks_df = new_tracks_df.reindex(columns = header_list)\n",
    "new_tracks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average number of weeks that a song is on the billboard is 12 weeks.\n",
    "# Create a for loop that determines if the row value in weeks_on_board column is greater than or equal to 12 and assign\n",
    "# it a new value of 0 and add it to the 'target' column and everything else change to a 1.\n",
    "for index in new_tracks_df.index:\n",
    "    if new_tracks_df.loc[index,'weeks_on_board']>=12:\n",
    "        new_tracks_df.loc[index,'target'] = 0\n",
    "    else:\n",
    "        new_tracks_df.loc[index,'target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tracks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops the weeks_on_board column since we are using the 'target' column \n",
    "# Drops ranking column to test how the ML models will perform without it\n",
    "target_tracks_df = new_tracks_df.drop([\"weeks_on_board\",\"ranking\"], axis=1)\n",
    "target_tracks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our features\n",
    "X = target_tracks_df.drop(columns = \"target\")\n",
    "#X = pd.get_dummies(X)\n",
    "\n",
    "# Create our target\n",
    "y = target_tracks_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the balance of our target values\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.10,\n",
    "                                                    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of examples we have in our train data set\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of examples we have in our test data set\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization: StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler with default parameters\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the scaler passing the training data and transforms the data\n",
    "# Returns it to variable \"train_scaled\"\n",
    "train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms test data the same way\n",
    "test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Nueral Network MLPClassifier ML Model for the target_tracks_df WITHOUT ranking\n",
    "    1. This dataframe excludes the number of weeks on the board and instead uses a binary outcome. This binary values was determined by if the weeks on board is greater than or equal to 12 then change to 0, else change to a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(solver='lbfgs', random_state=42, max_iter=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the training data to the network\n",
    "model.fit(train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the accuracy_score and declare the predictor variable\n",
    "y_pred = model.predict(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy score of our training data\n",
    "# max_iter results\n",
    "# 3000 = 73.03573322023115\n",
    "# 2500 = 73.03573322023115\n",
    "# 2000 = 72.9466652528894\n",
    "# 1000 = 72.72611600042413\n",
    "\n",
    "train_acc = accuracy_score(y_train, model.predict(train_scaled))*100\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy score of our test data\n",
    "# max_iter results\n",
    "# 3000 = 70.24809160305342\n",
    "# 2500 = 70.24809160305342\n",
    "# 2000 = 70.38167938931298\n",
    "# 1000 = 69.80916030534351\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred)*100\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "\n",
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {test_acc}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Random Forest ML Model for the target_tracks_df WITHOUT ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=4096, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "rf_model = rf_model.fit(train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the testing data.\n",
    "predictions = rf_model.predict(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy from training data\n",
    "accuracy_score(y_train, rf_model.predict(train_scaled))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy score from test data\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "print((acc_score)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance in the Random Forest model\n",
    "importances = rf_model.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the features by their importance\n",
    "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the KNearestRegressor ML Model for the target_tracks_df WITHOUT ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knr_model = KNeighborsRegressor(n_neighbors=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knr_model.fit(train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knr_mse = mean_squared_error(y_train, knr_model.predict(train_scaled))\n",
    "knr_mae = mean_absolute_error(y_train, knr_model.predict(train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mse = \",knr_mse,\" & mae = \",knr_mae,\" & rmse = \", sqrt(knr_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_knr_mse = mean_squared_error(y_test, knr_model.predict(test_scaled))\n",
    "test_knr_mae = mean_absolute_error(y_test, knr_model.predict(test_scaled))\n",
    "print(\"mse = \",test_knr_mse,\" & mae = \",test_knr_mae,\" & rmse = \", sqrt(test_knr_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the KNeighborsClassifier ML Model for the target_tracks_df WITHOUT ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc_model = KNeighborsClassifier(n_neighbors=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc_model.fit(train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc_mse = mean_squared_error(y_train, knc_model.predict(train_scaled))\n",
    "knc_mae = mean_absolute_error(y_train, knc_model.predict(train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mse = \",knc_mse,\" & mae = \",knc_mae,\" & rmse = \", sqrt(knc_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = mean_squared_error(y_test, knc_model.predict(test_scaled))\n",
    "test_mae = mean_absolute_error(y_test, knc_model.predict(test_scaled))\n",
    "print(\"mse = \",test_mse,\" & mae = \",test_mae,\" & rmse = \", sqrt(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
